{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "K43NJU5DdA3K",
   "metadata": {
    "id": "K43NJU5DdA3K"
   },
   "source": [
    "# EXPLORAÇÃO INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c0112a",
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1692019460049,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "11c0112a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "best_params = [142, 18, 3, 2]\n",
    "\n",
    "avg_price_per_sqmt_dict = {\n",
    "    'zona norte': 8.394846992222611,\n",
    "    'centro sul': 9.09851811665617,\n",
    "    'barreiro': 8.503898358767296,\n",
    "    'venda nova': 8.36755862148199,\n",
    "    'zona oeste': 8.843083032977592,\n",
    "    'zona noroeste': 7.96617976451086,\n",
    "    'zona leste': 8.816877975750044,\n",
    "    'pampulha': 8.626722968908846,\n",
    "    'zona nordeste': 8.66487771346997\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4OLUZr487UQM",
   "metadata": {
    "id": "4OLUZr487UQM"
   },
   "source": [
    "# Data Ingestion PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e542a5c",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0eba476",
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1692019932590,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "d0eba476"
   },
   "outputs": [],
   "source": [
    "# Atualizando a função para tratar valores inadequados antes da transformação\n",
    "def transform_currency_column_final(data, column_name):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Substituindo \".\" e depois substituindo \",\" por \".\"\n",
    "    data_copy[column_name] = data_copy[column_name].str.replace('R$ ', '', regex=False)\n",
    "    data_copy[column_name] = data_copy[column_name].str.replace('.', '', regex=False).str.replace(',', '.', regex=False)\n",
    "\n",
    "    # Convertendo para numérico e substituindo valores inválidos por NaN\n",
    "    data_copy[column_name] = pd.to_numeric(data_copy[column_name], errors='coerce')\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "# Atualizando a função para tratar valores inadequados na coluna AREA\n",
    "def transform_area_column_updated(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Removendo \"m²\" e tentando converter para int\n",
    "    data_copy['AREA'] = data_copy['AREA'].str.replace('m²', '', regex=False)\n",
    "    data_copy['AREA'] = pd.to_numeric(data_copy['AREA'], errors='coerce', downcast='integer')\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "# Atualizando as funções encapsuladas\n",
    "def transform_condo_column_final(data):\n",
    "    return transform_currency_column_final(data, 'CONDO')\n",
    "\n",
    "def transform_tax_column_final(data):\n",
    "    return transform_currency_column_final(data, 'TAX')\n",
    "\n",
    "\n",
    "\n",
    "# Função para substituir NaN por 0 nas colunas especificadas\n",
    "def replace_nan_with_zero(data, column_names):\n",
    "    data_copy = data.copy()\n",
    "    for col in column_names:\n",
    "        data_copy[col] = data_copy[col].fillna(0)\n",
    "    return data_copy\n",
    "\n",
    "# Função encapsulada para o pipeline\n",
    "def replace_nan_in_condo_and_tax(data):\n",
    "    return replace_nan_with_zero(data, ['CONDO', 'TAX'])\n",
    "\n",
    "\n",
    "# Função para remover linhas onde AREA é 0\n",
    "def drop_rows_with_zero_area(data):\n",
    "    return data[data['AREA'] != 0]\n",
    "\n",
    "\n",
    "# Função para remover linhas onde AREA é NaN\n",
    "def drop_rows_with_nan_area(data):\n",
    "    return data.dropna(subset=['AREA'])\n",
    "\n",
    "# Atualizando a função para tratar valores inadequados na coluna BATH_NO\n",
    "def transform_bath_no_column_updated(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Substituindo '5 ou mais' por '5'\n",
    "    data_copy['BATH_NO'] = data_copy['BATH_NO'].replace('5 ou mais', '5')\n",
    "\n",
    "    # Tentando converter para int e substituindo valores inválidos por NaN\n",
    "    data_copy['BATH_NO'] = pd.to_numeric(data_copy['BATH_NO'], errors='coerce', downcast='integer')\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "# Função para remover linhas onde BATH_NO é NaN\n",
    "def drop_rows_with_nan_bath_no(data):\n",
    "    return data.dropna(subset=['BATH_NO'])\n",
    "\n",
    "# Função para tratar a coluna PARKING_SPOTS\n",
    "def transform_parking_spots_column(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Substituindo '5 ou mais' por '5'\n",
    "    data_copy['PARKING_SPOTS'] = data_copy['PARKING_SPOTS'].replace('5 ou mais', '5')\n",
    "\n",
    "    # Tentando converter para int e substituindo valores inválidos por NaN\n",
    "    data_copy['PARKING_SPOTS'] = pd.to_numeric(data_copy['PARKING_SPOTS'], errors='coerce', downcast='integer')\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "# Função para remover linhas onde PARKING_SPOTS é NaN\n",
    "def drop_rows_with_nan_parking_spots(data):\n",
    "    return data.dropna(subset=['PARKING_SPOTS'])\n",
    "\n",
    "# Função para tratar a coluna ROOMS_NO\n",
    "def transform_rooms_no_column(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Substituindo '5 ou mais' por '5'\n",
    "    data_copy['ROOMS_NO'] = data_copy['ROOMS_NO'].replace('5 ou mais', '5')\n",
    "\n",
    "    # Tentando converter para int e substituindo valores inválidos por NaN\n",
    "    data_copy['ROOMS_NO'] = pd.to_numeric(data_copy['ROOMS_NO'], errors='coerce', downcast='integer')\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "# Função para remover linhas onde ROOMS_NO é NaN\n",
    "def drop_rows_with_nan_rooms_no(data):\n",
    "    return data.dropna(subset=['ROOMS_NO'])\n",
    "\n",
    "def create_apartment_details_dummies(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Removendo espaços em branco entre as vírgulas\n",
    "    data_copy['APARTMENT_DETAILS'] = data_copy['APARTMENT_DETAILS'].apply(lambda x: ','.join(item.strip() for item in x.split(',')) if isinstance(x, str) else x)\n",
    "\n",
    "    # Criando variáveis dummy para cada detalhe do apartamento\n",
    "    dummies = data_copy['APARTMENT_DETAILS'].str.get_dummies(sep=',')\n",
    "\n",
    "    # Renomeando as colunas das dummies\n",
    "    dummies = dummies.rename(lambda x: 'DETAIL_' + x, axis='columns')\n",
    "\n",
    "    # Concatenando o dataframe original com as colunas dummy\n",
    "    data_copy = pd.concat([data_copy, dummies], axis=1)\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "def drop_duplicated_rows(data):\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "def drop_duplicated_links(data):\n",
    "    return data.drop_duplicates(subset=['LINK'])\n",
    "\n",
    "def drop_zero_values_in_columns(data):\n",
    "    filtered_data = data[(data['ROOMS_NO'] != 0) & (data['AREA'] != 0) & (data['PRICE'] != 0) & (data['BATH_NO'] != 0)]\n",
    "    return filtered_data\n",
    "\n",
    "def drop_low_price_values(data):\n",
    "    return data[data['PRICE'] >= 50000]\n",
    "\n",
    "def drop_high_price_values(data):\n",
    "    return data[data['PRICE'] <= 5000000]\n",
    "\n",
    "def compute_avg_price_per_sqmt_by_region(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Calculando o preço médio por m² para cada bairro usando a coluna REGION\n",
    "    avg_price_per_sqmt_by_region = data_copy.groupby('REGION').apply(lambda x: x['PRICE'].sum() / x['AREA'].sum()).to_dict()\n",
    "\n",
    "    # Mapeando os valores médios para os registros no conjunto de dados\n",
    "    data_copy['AVG_PRICE_PER_SQMT_BY_REGION'] = data_copy['REGION'].map(avg_price_per_sqmt_by_region)\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "\n",
    "\n",
    "def create_region_dummies(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # Criando variáveis dummy para a coluna REGION e garantindo que elas sejam inteiros\n",
    "    dummies = pd.get_dummies(data_copy['REGION'], prefix='REGION', drop_first=True, dtype=int)\n",
    "\n",
    "    # Concatenando o dataframe original com as colunas dummy\n",
    "    data_copy = pd.concat([data_copy, dummies], axis=1)\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "def apply_log_transformations(data):\n",
    "    data_copy = data.copy()\n",
    "    data_copy['LOG_AREA'] = np.log1p(data_copy['AREA'])\n",
    "    data_copy['LOG_PRICE'] = np.log1p(data_copy['PRICE'])\n",
    "    data_copy['LOG_AVG_PRICE_PER_SQMT_BY_REGION'] = np.log1p(data_copy['AVG_PRICE_PER_SQMT_BY_REGION'])\n",
    "    return data_copy\n",
    "\n",
    "# Function to create the UTILS column by adding TAX and CONDO columns\n",
    "def create_utils_column(data):\n",
    "    data_copy = data.copy()\n",
    "    data_copy['UTILS'] = data_copy['TAX'] + data_copy['CONDO']\n",
    "    return data_copy\n",
    "\n",
    "def drop_non_log_columns(data):\n",
    "    data_copy = data.copy()\n",
    "    columns_to_drop = ['AREA', 'AVG_PRICE_PER_SQMT_BY_REGION']\n",
    "    data_copy = data_copy.drop(columns=columns_to_drop)\n",
    "    return data_copy\n",
    "\n",
    "# Function to split the data into train and test\n",
    "def split_data(data, test_size=0.2, random_state=42):\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xqps9Kk18h9u",
   "metadata": {
    "id": "Xqps9Kk18h9u"
   },
   "source": [
    "## Definição do pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "XsXXob5AOzYr",
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1692023480603,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "XsXXob5AOzYr",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_dataframe.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18992\\3009788212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Lê o arquivo CSV e carrega o DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"final_dataframe.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_dataframe.csv'"
     ]
    }
   ],
   "source": [
    "# Lê o arquivo CSV e carrega o DataFrame\n",
    "file_path = \"final_dataframe.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "EhXEcHzh8gSw",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1692023483132,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "EhXEcHzh8gSw"
   },
   "outputs": [],
   "source": [
    "# Redefining the transformation pipeline with the inclusion of the region dummy creation step\n",
    "final_transformation_pipeline = Pipeline(steps=[\n",
    "    ('transform_condo', FunctionTransformer(func=transform_condo_column_final, validate=False)),\n",
    "    ('transform_tax', FunctionTransformer(func=transform_tax_column_final, validate=False)),\n",
    "    ('replace_nan', FunctionTransformer(func=replace_nan_in_condo_and_tax, validate=False)),\n",
    "    ('transform_area', FunctionTransformer(func=transform_area_column_updated, validate=False)),\n",
    "    ('drop_nan_area', FunctionTransformer(func=drop_rows_with_nan_area, validate=False)),\n",
    "    ('transform_bath_no', FunctionTransformer(func=transform_bath_no_column_updated, validate=False)),\n",
    "    ('drop_nan_bath_no', FunctionTransformer(func=drop_rows_with_nan_bath_no, validate=False)),\n",
    "    ('transform_parking_spots', FunctionTransformer(func=transform_parking_spots_column, validate=False)),\n",
    "    ('drop_nan_parking_spots', FunctionTransformer(func=drop_rows_with_nan_parking_spots, validate=False)),\n",
    "    ('transform_rooms_no', FunctionTransformer(func=transform_rooms_no_column, validate=False)),\n",
    "    ('drop_nan_rooms_no', FunctionTransformer(func=drop_rows_with_nan_rooms_no, validate=False)),\n",
    "    ('details_dummies', FunctionTransformer(func=create_apartment_details_dummies, validate=False)),\n",
    "    ('drop_duplicates', FunctionTransformer(func=drop_duplicated_rows, validate=False)),\n",
    "    ('drop_duplicate_links', FunctionTransformer(func=drop_duplicated_links, validate=False)),\n",
    "    ('drop_zero_values', FunctionTransformer(func=drop_zero_values_in_columns, validate=False)),\n",
    "    ('drop_low_price', FunctionTransformer(func=drop_low_price_values, validate=False)),\n",
    "    ('drop_high_price', FunctionTransformer(func=drop_high_price_values, validate=False)),\n",
    "    ('compute_avg_price_sqmt', FunctionTransformer(func=compute_avg_price_per_sqmt_by_region, validate=False)),\n",
    "    ('region_dummies', FunctionTransformer(func=create_region_dummies, validate=False)),\n",
    "    ('log_transformations', FunctionTransformer(func=apply_log_transformations, validate=False)),\n",
    "    ('create_utils', FunctionTransformer(func=create_utils_column, validate=False)),\n",
    "    ('drop_non_log', FunctionTransformer(func=drop_non_log_columns, validate=False)),\n",
    "    ('split_data', FunctionTransformer(func=split_data, validate=False))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1520fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_per_sqmt_dict = train_data.drop_duplicates('REGION').set_index('REGION')['LOG_AVG_PRICE_PER_SQMT_BY_REGION'].to_dict()\n",
    "avg_price_per_sqmt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J7zyCeDD9Pn3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1692023485094,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "J7zyCeDD9Pn3",
    "outputId": "cd75516a-ee75-4f6d-f871-92688472e47d"
   },
   "outputs": [],
   "source": [
    "# Applying the entire transformation pipeline including split on the dataframe\n",
    "train_data, test_data = final_transformation_pipeline.transform(df)\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P51RRz73IM2q",
   "metadata": {
    "id": "P51RRz73IM2q"
   },
   "source": [
    "# FUNÇÕES: Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-iDR3MvtITK_",
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1692023495605,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "-iDR3MvtITK_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tBwzkh5nUyJI",
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1692023498607,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "tBwzkh5nUyJI"
   },
   "outputs": [],
   "source": [
    "best_params = [142, 18, 3, 2]\n",
    "\n",
    "# Feature and target selection\n",
    "features = [\n",
    "    \"ROOMS_NO\", \"BATH_NO\", \"PARKING_SPOTS\", \"DETAIL_Academia\",\n",
    "    \"DETAIL_Ar condicionado\", \"DETAIL_Armários na cozinha\", \"DETAIL_Armários no quarto\",\n",
    "    \"DETAIL_Churrasqueira\", \"DETAIL_Mobiliado\", \"DETAIL_Piscina\",\n",
    "    \"DETAIL_Varanda\", \"DETAIL_Área de serviço\", \"REGION_barreiro\", \"REGION_pampulha\",\n",
    "    \"REGION_venda nova\", \"REGION_centro sul\", \"REGION_zona leste\", \"REGION_zona nordeste\",\n",
    "    \"REGION_zona noroeste\", \"REGION_zona norte\", \"REGION_zona oeste\", \"LOG_AREA\",\n",
    "    \"LOG_AVG_PRICE_PER_SQMT_BY_REGION\", \"UTILS\"\n",
    "]\n",
    "target = [\"LOG_PRICE\"]\n",
    "\n",
    "\n",
    "# Defining the best regressor with the provided parameters\n",
    "best_regressor = RandomForestRegressor(\n",
    "    n_estimators=int(best_params[0]),\n",
    "    max_depth=int(best_params[1]) if best_params[1] is not None else None,\n",
    "    min_samples_split=int(best_params[2]),\n",
    "    min_samples_leaf=int(best_params[3]),\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_features_and_target(data, features, target):\n",
    "    \"\"\"\n",
    "    Retorna um dataframe contendo apenas as colunas especificadas em 'features' e 'target'.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): dataframe de entrada.\n",
    "    - features (list of str): lista das colunas de características.\n",
    "    - target (list of str): lista contendo a coluna alvo.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: dataframe contendo apenas as colunas especificadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_columns = features + target\n",
    "    return data[selected_columns]\n",
    "\n",
    "def train_model(X, y, regressor):\n",
    "    \"\"\"\n",
    "    Treina o modelo com os dados fornecidos.\n",
    "\n",
    "    Args:\n",
    "    - X (pd.DataFrame): dataframe de características.\n",
    "    - y (pd.Series): série alvo.\n",
    "    - regressor (estimator): modelo a ser treinado.\n",
    "\n",
    "    Returns:\n",
    "    - regressor: modelo treinado.\n",
    "    \"\"\"\n",
    "    regressor.fit(X, y)\n",
    "    return regressor\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Avalia o modelo utilizando o RMSE e o MAE.\n",
    "\n",
    "    Args:\n",
    "    - model (estimator): modelo treinado.\n",
    "    - X (pd.DataFrame): dataframe de características.\n",
    "    - y (pd.Series): série alvo.\n",
    "\n",
    "    Returns:\n",
    "    - rmse (float): Root Mean Squared Error.\n",
    "    - mae (float): Mean Absolute Error.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Calcular o RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    \n",
    "    # Calcular o MAE\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    \n",
    "    return rmse, mae\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JVwAqcW7VmW5",
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1692023503105,
     "user": {
      "displayName": "Vitor Sampaio",
      "userId": "10293578718336417452"
     },
     "user_tz": 180
    },
    "id": "JVwAqcW7VmW5"
   },
   "outputs": [],
   "source": [
    "# Criar o pipeline\n",
    "modeling_pipeline = Pipeline(steps=[\n",
    "    ('select_columns', FunctionTransformer(func=select_features_and_target, validate=False, kw_args={\"features\": features, \"target\": target})),\n",
    "    ('train_model', FunctionTransformer(func=train_model, validate=False, kw_args={\"regressor\": best_regressor})),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "X_train, y_train = train_data[features], train_data[target]\n",
    "trained_model = modeling_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b98b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo\n",
    "X_test, y_test = test_data[features], test_data[target]\n",
    "evaluate_model(trained_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cbb4a8",
   "metadata": {},
   "source": [
    "# Fazendo a previsão em novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and target selection\n",
    "features_predict = [\n",
    "    \"ROOMS_NO\", \"BATH_NO\", \"PARKING_SPOTS\", \"DETAIL_Academia\",\n",
    "    \"DETAIL_Ar condicionado\", \"DETAIL_Armários na cozinha\", \"DETAIL_Armários no quarto\",\n",
    "    \"DETAIL_Churrasqueira\", \"DETAIL_Mobiliado\", \"DETAIL_Piscina\",\n",
    "    \"DETAIL_Varanda\", \"DETAIL_Área de serviço\", \"REGION_barreiro\", \"REGION_pampulha\",\n",
    "    \"REGION_venda nova\", \"REGION_centro sul\", \"REGION_zona leste\", \"REGION_zona nordeste\",\n",
    "    \"REGION_zona noroeste\", \"REGION_zona norte\", \"REGION_zona oeste\", \"LOG_AREA\",\n",
    "    \"LOG_AVG_PRICE_PER_SQMT_BY_REGION\", \"UTILS\"\n",
    "]\n",
    "target = [\"LOG_PRICE\"]\n",
    "\n",
    "\n",
    "\n",
    "def score_new_data(new_data, model, pipeline):\n",
    "    \"\"\"\n",
    "    Pontua novos dados usando o modelo treinado e o pipeline de transformação.\n",
    "\n",
    "    Args:\n",
    "    - new_data (pd.DataFrame): Novos dados para pontuar.\n",
    "    - model (estimator): Modelo treinado.\n",
    "    - pipeline (Pipeline): Pipeline de transformação.\n",
    "\n",
    "    Returns:\n",
    "    - scored_data (pd.DataFrame): DataFrame com previsões do modelo e dados transformados.\n",
    "    \"\"\"\n",
    "    # Transformar os dados\n",
    "    transformed_data = pipeline.transform(new_data)\n",
    "    \n",
    "    # Salvar uma cópia dos dados transformados\n",
    "    transformed_copy = transformed_data.copy()\n",
    "    \n",
    "    # Selecionar apenas as características desejadas\n",
    "    transformed_data = transformed_data[features_predict]\n",
    "    \n",
    "    # Fazer previsões\n",
    "    predictions = model.predict(transformed_data)\n",
    "    \n",
    "    # Criar uma cópia do DataFrame original com uma nova coluna \"predictions\"\n",
    "    transformed_copy[\"predictions\"] = np.exp(predictions) - 1\n",
    "        \n",
    "    return transformed_copy\n",
    "\n",
    "def fill_avg_price_per_sqmt(new_data, avg_price_dict):\n",
    "    # Assumindo que sua coluna de bairro nos novos dados também é chamada de \"REGION\"\n",
    "    new_data['LOG_AVG_PRICE_PER_SQMT_BY_REGION'] = new_data['REGION'].map(avg_price_dict)\n",
    "    return new_data\n",
    "\n",
    "def apply_log_area (data):\n",
    "    data_copy = data.copy()\n",
    "    data_copy['LOG_AREA'] = np.log1p(data_copy['AREA'])\n",
    "    return data_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28861d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina o pipeline completo\n",
    "scoring_pipeline = Pipeline(steps=[\n",
    "    ('transform_condo', FunctionTransformer(func=transform_condo_column_final, validate=False)),\n",
    "    ('transform_tax', FunctionTransformer(func=transform_tax_column_final, validate=False)),\n",
    "    ('fill_avg_price', FunctionTransformer(func=fill_avg_price_per_sqmt, kw_args={'avg_price_dict': avg_price_per_sqmt_dict}, validate=False)),\n",
    "    ('replace_nan', FunctionTransformer(func=replace_nan_in_condo_and_tax, validate=False)),\n",
    "    ('transform_area', FunctionTransformer(func=transform_area_column_updated, validate=False)),\n",
    "    ('drop_nan_area', FunctionTransformer(func=drop_rows_with_nan_area, validate=False)),\n",
    "    ('transform_bath_no', FunctionTransformer(func=transform_bath_no_column_updated, validate=False)),\n",
    "    ('drop_nan_bath_no', FunctionTransformer(func=drop_rows_with_nan_bath_no, validate=False)),\n",
    "    ('transform_parking_spots', FunctionTransformer(func=transform_parking_spots_column, validate=False)),\n",
    "    ('drop_nan_parking_spots', FunctionTransformer(func=drop_rows_with_nan_parking_spots, validate=False)),\n",
    "    ('transform_rooms_no', FunctionTransformer(func=transform_rooms_no_column, validate=False)),\n",
    "    ('drop_nan_rooms_no', FunctionTransformer(func=drop_rows_with_nan_rooms_no, validate=False)),\n",
    "    ('details_dummies', FunctionTransformer(func=create_apartment_details_dummies, validate=False)),\n",
    "    ('drop_duplicates', FunctionTransformer(func=drop_duplicated_rows, validate=False)),\n",
    "    ('drop_duplicate_links', FunctionTransformer(func=drop_duplicated_links, validate=False)),\n",
    "    ('drop_zero_values', FunctionTransformer(func=drop_zero_values_in_columns, validate=False)),\n",
    "    ('create_utils', FunctionTransformer(func=create_utils_column, validate=False)),\n",
    "    ('region_dummies', FunctionTransformer(func=create_region_dummies, validate=False)),\n",
    "    ('log_transformations', FunctionTransformer(func=apply_log_area, validate=False)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083be9c1",
   "metadata": {},
   "source": [
    "### Testando com os dados scrapados recentemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"dados_detalhados_olx.csv\")\n",
    "new_data.head(1)\n",
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6a304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data_predictions = score_new_data(new_data, trained_model, scoring_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_data_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(new_data_predictions[\"predictions\"], new_data_predictions[\"PRICE\"], label=\"Data\")\n",
    "plt.plot([new_data_predictions[\"predictions\"].min(), new_data_predictions[\"predictions\"].max()], [new_data_predictions[\"predictions\"].min(), new_data_predictions[\"predictions\"].max()], color=\"red\", label=\"Linear Fit\")\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Predictions vs. Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(new_data_predictions[\"PRICE\"], new_data_predictions[\"predictions\"])\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a8dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
